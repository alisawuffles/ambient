{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139af885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/xlab/alisaliu/ambient\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name != 'ambient':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e396c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mmfs1/home/alisaliu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mmfs1/home/alisaliu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyinflect\n",
    "import spacy\n",
    "from generation.gpt3_generation import request\n",
    "from evaluation.distractors import create_distractor\n",
    "from evaluation.conceptnet_utils import word_to_term, term_to_word, get_nodes\n",
    "from utils.utils import flatten_list_of_lists\n",
    "import requests\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d0f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The Department of Education is responsible for Title IX.'\n",
    "doc_dep = nlp(text)\n",
    "for i in range(len(doc_dep)):\n",
    "    token = doc_dep[i]\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33158a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('annotation/AmbiEnt/validated_examples.jsonl', lines=True)\n",
    "test_df = test_df[test_df['premise_ambiguous'] ^ test_df['hypothesis_ambiguous']]\n",
    "\n",
    "perturbations = {}\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df.index)):\n",
    "    ambiguous_sent_key = 'premise' if row['premise_ambiguous'] else 'hypothesis'\n",
    "    distractor = create_distractor(row[ambiguous_sent_key], method='replace_word')\n",
    "    perturbations[row[ambiguous_sent_key]] = distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c2c3d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/en/media\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/c/en/consumer_durables_apparel'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_word = 'jobs'\n",
    "print(word_to_term(replaced_word))\n",
    "category_terms = get_nodes(word_to_term(replaced_word), relation='IsA', node_type='end')\n",
    "\n",
    "all_related_terms = {}\n",
    "for category_term, weight in category_terms.items():\n",
    "    related_terms = get_nodes(category_term, relation=['IsA'], node_type='start')\n",
    "    all_related_terms.update({k:v*weight for k,v in related_terms.items() if term_to_word(k) != replaced_word})\n",
    "\n",
    "if all_related_terms:\n",
    "    new_term = max(all_related_terms, key=all_related_terms.get)\n",
    "new_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6792320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(term, relation, node_type=None):\n",
    "    \"\"\"\n",
    "    term: e.g., \"/c/en/ask\"\n",
    "    relations: either a relation or a list of relations\n",
    "    node_type: either 'start' or 'end'\n",
    "    \"\"\"\n",
    "    if isinstance(relation, str):\n",
    "        relation = [relation]\n",
    "    if isinstance(node_type, str):\n",
    "        node_type = [node_type]\n",
    "    elif not node_type:\n",
    "        node_type = ['start', 'end']\n",
    "        \n",
    "    obj = requests.get(f'http://api.conceptnet.io{term}?limit=1000').json()\n",
    "    nodes = {}\n",
    "    for e in obj['edges']:\n",
    "        if e['rel']['label'] in relation:\n",
    "            for n in node_type:\n",
    "                node = e[n]\n",
    "                if node['term'] != term: #and '_' not in node['term']:\n",
    "                    nodes[node['term']] = e['weight']\n",
    "    \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def replace_verb(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            new_verb = random.choice(verbs)\n",
    "            sentence = sentence.replace(token.text, new_verb, 1)\n",
    "            break\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464af9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love eating pizza\"\n",
    "replace_verb(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_VERBS = []\n",
    "for synset in wordnet.all_synsets('v'):\n",
    "    for lemma in synset.lemmas():\n",
    "        verb_text = lemma.name()\n",
    "        if '_' not in verb_text and verb_text.islower():\n",
    "            ALL_VERBS.append(verb_text)\n",
    "\n",
    "ALL_VERBS = list(set(ALL_VERBS))  # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a63522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_token(i):\n",
    "    return f'<extra_id_{i}>'\n",
    "\n",
    "def mask_and_replace(sentences, verbose=False):\n",
    "    # mask first noun, verb, or adjective in sentence\n",
    "    masked_sentences = []\n",
    "    bad_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split(' ')\n",
    "        sentence_len = len(sentence_words)\n",
    "        mask_idxs = sorted(random.sample(range(sentence_len), int(np.ceil(0.15*sentence_len))))\n",
    "        masked_tokens = []\n",
    "        \n",
    "        for i, mask_idx in enumerate(mask_idxs):\n",
    "            masked_tokens.append(sentence_words[mask_idx])\n",
    "            sentence_words[mask_idx] = get_mask_token(i)            \n",
    "            \n",
    "        masked_sentence = ' '.join(sentence_words)\n",
    "        masked_sentences.append(masked_sentence)\n",
    "        bad_words.append(masked_tokens)\n",
    "    \n",
    "    print(masked_sentences)\n",
    "    print(bad_words)\n",
    "    # replace with T5 prediction\n",
    "    input_ids = tokenizer(masked_sentences, return_tensors=\"pt\", padding=True).input_ids\n",
    "    bad_words_ids = [flatten_list_of_lists(tokenizer(l, add_special_tokens=False).input_ids) for l in bad_words]\n",
    "    print(bad_words_ids)\n",
    "    \n",
    "    sequence_ids = model.generate(input_ids, do_sample=True, top_k=0, bad_words_ids=bad_words_ids)\n",
    "    print(sequence_ids)\n",
    "    sequences = tokenizer.batch_decode(sequence_ids)\n",
    "    print(sequences)\n",
    "    \n",
    "    return fill_masks(masked_sentences, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c5d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_and_replace(['We will be able to increase our production by 10 percent if we can get the additional machinery'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362941f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('annotation/AmbiEnt/validated_examples.jsonl', lines=True)\n",
    "test_df = test_df[test_df['premise_ambiguous'] ^ test_df['hypothesis_ambiguous']]\n",
    "\n",
    "perturbations = {}\n",
    "ambiguous_sentences = []\n",
    "for i, row in test_df.iterrows():\n",
    "    ambiguous_sent_key = 'premise' if row['premise_ambiguous'] else 'hypothesis'\n",
    "    ambiguous_sentences.append(row[ambiguous_sent_key])\n",
    "\n",
    "new_sentences = mask_and_replace(ambiguous_sentences)\n",
    "for o, n in zip(ambiguous_sentences, new_sentences):\n",
    "    perturbations[o] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ead51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_replace(sentences):\n",
    "    masked_sentences = []\n",
    "    bad_words = []\n",
    "    for sentence in sentences:\n",
    "        doc_dep = nlp(sentence)\n",
    "        for token in doc_dep:\n",
    "            if token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
    "                print(token, token.pos_)\n",
    "                sentence = sentence.replace(token.text, get_mask_token(0))\n",
    "                break\n",
    "        \n",
    "        masked_sentences.append(sentence)\n",
    "        bad_words.append(token.text)\n",
    "    \n",
    "    # replace with T5 prediction\n",
    "    input_ids = tokenizer(masked_sentences, return_tensors=\"pt\", padding=True).input_ids\n",
    "    bad_words_ids = [tokenizer(l, add_special_tokens=False).input_ids for l in bad_words]\n",
    "    \n",
    "    sequence_ids = model.generate(input_ids, do_sample=True, top_k=100, bad_words_ids=bad_words_ids)\n",
    "    sequences = tokenizer.batch_decode(sequence_ids)\n",
    "    new_spans = [s.split(get_mask_token(0))[1].split(get_mask_token(1))[0].replace('</s>', '').strip() for s in sequences]\n",
    "    new_sentences = [sent.replace(get_mask_token(0), s).capitalize() for sent, s in zip(masked_sentences, new_spans)]\n",
    "\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393889bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_and_replace([\"I'm not trying to downplay the seriousness of the situation.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bde159",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentence = 'People should be aware <extra_id_0> the dangers of not properly securing their <extra_id_1>'\n",
    "s = '<pad><extra_id_0> of<extra_id_1> homes.</s>'\n",
    "mask_idx = 0\n",
    "while True:\n",
    "    mask_token = get_mask_token(mask_idx)\n",
    "    if mask_token in s:\n",
    "        new_span = s.split(mask_token)[1].split(get_mask_token(mask_idx+1))[0].replace('</s>', '').strip()\n",
    "        masked_sentence = masked_sentence.replace(get_mask_token(mask_idx), new_span)\n",
    "        mask_idx += 1\n",
    "    else:\n",
    "        break\n",
    "masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36750dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_masks(masked_sentences, t5_outputs):\n",
    "    new_sentences = []\n",
    "    for masked_sentence, t5_output in zip(masked_sentences, t5_outputs):\n",
    "        mask_idx = 0\n",
    "        while True:\n",
    "            mask_token = get_mask_token(mask_idx)\n",
    "            if mask_token in t5_output:\n",
    "                new_span = t5_output.split(mask_token)[1].split(get_mask_token(mask_idx+1))[0].replace('</s>', '').strip()\n",
    "                masked_sentence = masked_sentence.replace(get_mask_token(mask_idx), new_span)\n",
    "                mask_idx += 1\n",
    "            else:\n",
    "                break\n",
    "        new_sentences.append(masked_sentence.capitalize())\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c2fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
