{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb2a371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/xlab/alisaliu/ambient\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name != 'ambient':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845c6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from utils.constants import OPENAI_API_KEY\n",
    "from generation.gpt3_generation import request\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import time\n",
    "from scipy.special import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b84d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6zqpzOWeKOFVJHtBwZmClpU29rpUW at 0x15316f3b7770> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": {\n",
       "        \"text_offset\": [\n",
       "          37\n",
       "        ],\n",
       "        \"token_logprobs\": [\n",
       "          -1.4833127\n",
       "        ],\n",
       "        \"tokens\": [\n",
       "          \" I\"\n",
       "        ],\n",
       "        \"top_logprobs\": [\n",
       "          {\n",
       "            \"\\n\": -3.6776233,\n",
       "            \"\\n\\n\": -6.5934963,\n",
       "            \" \\\"\": -6.1570916,\n",
       "            \" .\": -6.8958473,\n",
       "            \" A\": -3.7638068,\n",
       "            \" According\": -7.0056334,\n",
       "            \" Actually\": -7.0313334,\n",
       "            \" After\": -7.0600767,\n",
       "            \" All\": -6.0441585,\n",
       "            \" An\": -6.0493956,\n",
       "            \" And\": -4.745432,\n",
       "            \" Another\": -7.0965858,\n",
       "            \" Apparently\": -5.5949655,\n",
       "            \" Are\": -6.7459054,\n",
       "            \" As\": -5.7931533,\n",
       "            \" At\": -6.0628023,\n",
       "            \" Both\": -7.1316915,\n",
       "            \" But\": -5.0131383,\n",
       "            \" By\": -7.0938478,\n",
       "            \" Can\": -5.4643717,\n",
       "            \" Could\": -6.8957996,\n",
       "            \" Did\": -6.2742667,\n",
       "            \" Do\": -5.077976,\n",
       "            \" Don\": -5.9732404,\n",
       "            \" For\": -7.2485676,\n",
       "            \" Fortunately\": -6.4536376,\n",
       "            \" From\": -6.640299,\n",
       "            \" Get\": -6.528694,\n",
       "            \" Good\": -6.5781794,\n",
       "            \" Had\": -7.264363,\n",
       "            \" Have\": -6.9435945,\n",
       "            \" He\": -2.5439208,\n",
       "            \" Her\": -6.074485,\n",
       "            \" Here\": -6.9672112,\n",
       "            \" His\": -5.4849033,\n",
       "            \" Hopefully\": -7.0673313,\n",
       "            \" How\": -6.6173744,\n",
       "            \" I\": -1.4833127,\n",
       "            \" If\": -4.9402514,\n",
       "            \" In\": -5.744983,\n",
       "            \" Is\": -6.2040286,\n",
       "            \" It\": -2.1219733,\n",
       "            \" Its\": -6.671061,\n",
       "            \" Just\": -6.110315,\n",
       "            \" Let\": -6.2969375,\n",
       "            \" Look\": -6.513608,\n",
       "            \" Looks\": -6.2978907,\n",
       "            \" Luckily\": -5.922061,\n",
       "            \" Maybe\": -5.5820546,\n",
       "            \" Mr\": -6.944216,\n",
       "            \" My\": -4.4778953,\n",
       "            \" No\": -5.268938,\n",
       "            \" Nobody\": -6.524743,\n",
       "            \" Not\": -5.911439,\n",
       "            \" Nothing\": -7.0071735,\n",
       "            \" Now\": -6.237572,\n",
       "            \" Oh\": -6.2590823,\n",
       "            \" On\": -7.0811276,\n",
       "            \" One\": -4.9180613,\n",
       "            \" Or\": -6.278472,\n",
       "            \" Our\": -6.283963,\n",
       "            \" Perhaps\": -7.322772,\n",
       "            \" Please\": -5.543032,\n",
       "            \" Police\": -6.292191,\n",
       "            \" Probably\": -6.0598373,\n",
       "            \" Right\": -6.771416,\n",
       "            \" See\": -6.95148,\n",
       "            \" She\": -3.555611,\n",
       "            \" So\": -5.633827,\n",
       "            \" Some\": -6.1016407,\n",
       "            \" Somebody\": -5.4573054,\n",
       "            \" Someone\": -4.6095047,\n",
       "            \" Something\": -6.9943247,\n",
       "            \" Sorry\": -5.690004,\n",
       "            \" T\": -7.2394085,\n",
       "            \" Thank\": -6.1250315,\n",
       "            \" That\": -4.225864,\n",
       "            \" The\": -2.5917168,\n",
       "            \" There\": -4.1220107,\n",
       "            \" They\": -4.1618433,\n",
       "            \" This\": -5.248216,\n",
       "            \" Three\": -7.0358768,\n",
       "            \" Too\": -7.280673,\n",
       "            \" Two\": -5.6119175,\n",
       "            \" Unfortunately\": -6.328501,\n",
       "            \" Very\": -7.263756,\n",
       "            \" We\": -3.0506942,\n",
       "            \" Well\": -7.008207,\n",
       "            \" What\": -6.036462,\n",
       "            \" When\": -5.6823087,\n",
       "            \" Where\": -7.21496,\n",
       "            \" While\": -6.8942943,\n",
       "            \" Who\": -7.1023254,\n",
       "            \" Whoever\": -7.1372128,\n",
       "            \" Why\": -7.309916,\n",
       "            \" Will\": -7.238388,\n",
       "            \" Would\": -7.3219547,\n",
       "            \" You\": -4.038876,\n",
       "            \" Your\": -5.9445367,\n",
       "            \"bytes:\\\\xe2\\\\x80\": -5.806516\n",
       "          }\n",
       "        ]\n",
       "      },\n",
       "      \"text\": \" I\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1680199135,\n",
       "  \"id\": \"cmpl-6zqpzOWeKOFVJHtBwZmClpU29rpUW\",\n",
       "  \"model\": \"ada\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 1,\n",
       "    \"prompt_tokens\": 12,\n",
       "    \"total_tokens\": 13\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = request(\n",
    "    prompt=\"\\\"I'm afraid the cat was hit by a car.\",\n",
    "    top_p=1.0,\n",
    "    model='ada',\n",
    "    logprobs=100,\n",
    "    temperature=0,\n",
    "    max_tokens=1,\n",
    "    logit_bias={50256: -100},\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ff6eadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00797717, 0.00112867, 0.0026312 , 0.00100971, 0.00339968,\n",
       "       0.00073704, 0.00079316, 0.00078073, 0.0173743 , 0.00225423,\n",
       "       0.01808033, 0.01629684, 0.00149102, 0.00102277, 0.03185595,\n",
       "       0.00574251, 0.00398688, 0.0007681 , 0.00264501, 0.00209251,\n",
       "       0.00093773, 0.00163573, 0.00259616, 0.0037982 , 0.00472309,\n",
       "       0.00107532, 0.01110428, 0.00260387, 0.00414486, 0.00266545,\n",
       "       0.00098561, 0.00815616, 0.00302042, 0.00376908, 0.0281969 ,\n",
       "       0.00095076, 0.01266624, 0.00175628, 0.00283941, 0.00695082,\n",
       "       0.00095768, 0.00247569, 0.002566  , 0.00462711, 0.00298851,\n",
       "       0.00080127, 0.00217987, 0.00092588, 0.00249726, 0.00113037,\n",
       "       0.00205227, 0.00080046, 0.25303365, 0.02586856, 0.00292209,\n",
       "       0.00105088, 0.00162928, 0.00145723, 0.00236255, 0.00088659,\n",
       "       0.08352269, 0.00100867, 0.00205422, 0.00208105, 0.00213348,\n",
       "       0.00106754, 0.00074596, 0.00436581, 0.00131118, 0.002064  ,\n",
       "       0.00475658, 0.00335456, 0.00155062, 0.00101126, 0.00407519,\n",
       "       0.00091806, 0.00356745, 0.00089149, 0.00092335, 0.00152705,\n",
       "       0.13360155, 0.00082027, 0.00141308, 0.00127816, 0.00419872,\n",
       "       0.08761169, 0.00165405, 0.00243953, 0.00112861, 0.00098114,\n",
       "       0.00586275, 0.00210133, 0.05278046, 0.00107599, 0.00969299,\n",
       "       0.00073643, 0.00741642, 0.0019904 , 0.01964768, 0.0007812 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(list(res['choices'][0]['logprobs']['top_logprobs'][0].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43f6607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = request(\n",
    "    model='text-davinci-003',\n",
    "    prompt=\"Q: The little girl's feet were too big for her mother's shoes. This can only mean: The little girl's feet were too big for her mother's shoes to fit. True or False?\\nA:\",\n",
    "    max_tokens=1,\n",
    "    logit_bias={\"50256\": -100},\n",
    "    logprobs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4e1951c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x1538cfd00040> JSON: {\n",
       "  \"\\t\": -18.955574,\n",
       "  \"\\n\": -12.732953,\n",
       "  \"\\n\\n\": -22.193111,\n",
       "  \" \": -12.071195,\n",
       "  \"  \": -17.894869,\n",
       "  \"   \": -20.000378,\n",
       "  \"    \": -20.962934,\n",
       "  \"     \": -22.80294,\n",
       "  \" \\\"\": -19.784613,\n",
       "  \" '\": -21.620714,\n",
       "  \" (\": -20.813599,\n",
       "  \" *\": -21.90518,\n",
       "  \" **\": -22.113203,\n",
       "  \" -\": -21.355145,\n",
       "  \" :\": -22.292599,\n",
       "  \" <\": -22.036457,\n",
       "  \" A\": -20.581108,\n",
       "  \" All\": -22.865637,\n",
       "  \" Answer\": -22.266504,\n",
       "  \" Boolean\": -22.634678,\n",
       "  \" Correct\": -20.075634,\n",
       "  \" FALSE\": -22.456907,\n",
       "  \" False\": -11.134022,\n",
       "  \" If\": -22.95495,\n",
       "  \" It\": -19.72489,\n",
       "  \" T\": -21.046103,\n",
       "  \" TRUE\": -11.851189,\n",
       "  \" That\": -18.001345,\n",
       "  \" The\": -16.278666,\n",
       "  \" This\": -15.263261,\n",
       "  \" Tru\": -18.309423,\n",
       "  \" True\": -0.0025240008,\n",
       "  \" Truth\": -20.967466,\n",
       "  \" Yes\": -18.296894,\n",
       "  \" `\": -22.27556,\n",
       "  \" false\": -22.324657,\n",
       "  \" true\": -12.0875025,\n",
       "  \" \\u200b\": -20.921247,\n",
       "  \"-\": -22.914064,\n",
       "  \":\": -22.305874,\n",
       "  \"False\": -16.979778,\n",
       "  \"T\": -21.734798,\n",
       "  \"TR\": -20.003218,\n",
       "  \"The\": -20.994223,\n",
       "  \"This\": -20.585257,\n",
       "  \"True\": -5.997864,\n",
       "  \"Yes\": -21.776144,\n",
       "  \"true\": -15.031981,\n",
       "  \"\\u00a0\": -17.095152,\n",
       "  \"\\u200b\": -22.107714\n",
       "}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['choices'][0]['logprobs']['top_logprobs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f298a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e544970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57938ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Is this True or False? Cats are animals. Answer:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "output_input_ids = tokenizer.encode('True', return_tensors='pt').to('cuda')\n",
    "outputs = model(input_ids=input_ids, labels=output_input_ids)\n",
    "# generate_ids = model.generate(input_ids, max_length=100, do_sample=True, num_return_sequences=5)\n",
    "# conts = tokenizer.batch_decode(generate_ids, skip_special_token=True)\n",
    "# conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d441833e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10998,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fd37e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c5167251",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "baa7b21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(465, device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8f6394ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769989c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Taking care of her daughter is a good idea.',\n",
       " ' She always urged her dad to bring her baby to the house.',\n",
       " ' To her mum, she has to buy a coat for work.',\n",
       " ' I sat down with two friends during a lunch break.',\n",
       " ' She takes over the company with a little boy.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l.replace('<pad>', '').split('.')[0]+'.' for l in conts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1626ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(1877)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = model._prepare_decoder_input_ids_for_generation(input_ids.size(0))\n",
    "logits = model(input_ids, decoder_input_ids=decoder_input_ids, output_hidden_states=False).logits[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159125ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dca6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits[0,10998])\n",
    "print(logits[0,10747])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(logits[:,10998] > logits[:,10747]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df175c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0,10747]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511870ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ast.literal_eval('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = request(\n",
    "    prompt = \"In each example, you will be given some context and a claim, where the correctness of the claim is affected by some ambiguity in the context. Enumerate two or three interpretations of the context that lead to different judgments about the claim.\\n\\nContext: The company has been in trouble for some time, and its stock prices have been falling steadily.\\nClaim: The company's stock prices have been falling steadily because the company has been in trouble for some time. Given the context alone, is this claim true, false, or inconclusive?\\nWe don't know, because the context can be interpreted in many different ways:\\n1. The company has been in trouble for some time, which has caused its stock prices to fall steadily. Then the claim is true.\\n2. The company has been in trouble for some time, and also its stock prices have been falling steadily. Then the claim is inconclusive.\\n\\nContext: This is not to say that she is not a good person.\\nClaim: She is not a good person. Given the context alone, is this claim true, false, or inconclusive?\\nWe don't know, because the context can be interpreted in many different ways:\\n1. This is not necessarily to say that she is not a good person. Then the claim is inconclusive.\\n2. This is not to say that she is not a good person, because she is. Then the claim is false.\\n\\nContext: All things being equal, she would have chosen the job in New York.\\nClaim: She chose the job in New York. Given the context alone, is this claim true, false, or inconclusive?\\nWe don't know, because the context can be interpreted in many different ways:\\n1. If the situation were different such that all things were equal, she would have chosen the job in New York. Then the claim is false.\\n2. All things being equal, she would have chosen the job in New York, and she may have. Then the claim is inconclusive.\\n\\nContext: The man was charged with first degree murder.\\nClaim: The man was charged with second degree murder. Given the context alone, is this claim true, false, or inconclusive?\\nWe don't know, because the context can be interpreted in many different ways:\\n1. The man was charged only with first degree murder. Then the claim is false.\\n2. The man was charged with first degree murder, possibly among other charges. Then the claim is inconclusive.\\n\\nContext: They are not the kind of people who would take advantage of a situation like this.\\nClaim: Taking advantage of a situation like this is not the right thing to do. Given the context alone, is this claim true, false, or inconclusive?\\nWe don't know, because the context can be interpreted in many different ways:\\n1. They are not the kind of people who would make unfair use of a situation like this. Then the claim is true.\\n2. They are not the kind of people who would use the opportunity offered by a situation like this. Then the claim is inconclusive.\\n\\nContext: The fact that he was able to do it without any training is a testament to his natural ability.\\nClaim: The fact that he was able to do it without any training is a testament to his determination. Given the context alone, is this claim true, false, or inconclusive?\\nWe don't know, because the context can be interpreted in many different ways:\\n1.\",\n",
    "    model='text-davinci-003',\n",
    "    temperature=0,\n",
    "    stop='\\n\\n',The company doing well did not lead to the employees being happy.\n",
    "    return_only_text=True,\n",
    "    logit_bias={\"50256\": -100},\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[o[3:] for o in response.lstrip().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "llama_path = Path('/gscratch/scrubbed/alisaliu/llama_hf')\n",
    "tokenizer = LlamaTokenizer.from_pretrained(llama_path / 'tokenizer')\n",
    "model = LlamaForCausalLM.from_pretrained(llama_path / \"llama-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input_ids = tokenizer.encode('He is a ', return_tensors='pt').to(model.device)\n",
    "output_input_ids = tokenizer.encode('boy', return_tensors='pt').to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bc650e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 4096, padding_idx=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32bfa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'A cat is an animal. True or False? Answer:'\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "logits = model(**inputs).logits[:,-1,:]\n",
    "# st = time.time()\n",
    "# full_ids = model.generate(input_ids, do_sample=True, max_new_tokens=20, num_return_sequences=128)\n",
    "# et = time.time()\n",
    "# print(et - st)\n",
    "# input_ids = input_ids.repeat(full_ids.shape[0], 1)\n",
    "# generate_ids = [out[len(inp):] for inp, out in zip(input_ids, full_ids)]\n",
    "# tokenizer.batch_decode(generate_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5819385d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7185, 0.7185], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(logits, dim=-1)\n",
    "probs[:, 5852] + probs[:, 7700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79c15dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_tokens = {True: 5852, False: 7700}\n",
    "TF =  (logits[:, TF_tokens[True]] > logits[:, TF_tokens[False]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51d1ca4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits, dim=-1)\n",
    "prob_mass = (probs[:, TF_tokens[True]] + probs[:, TF_tokens[False]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2338c0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7184631824493408]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb790399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3199064413706463"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1 input, 64 sequences: 46\n",
    "1 input, 128 sequences: 71s\n",
    "1 input, 256 sequences: 108s\n",
    "3 inputs, 64 sequences: 113.1360330581665\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "300044a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4565217391304346"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "113/46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749181ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   450,  5637,  ...,  9796, 29889,   450],\n",
       "        [    1,   450,  5637,  ...,  6963,   902,  2217],\n",
       "        [    1,   450,  5637,  ...,   471,   263,  1487],\n",
       "        ...,\n",
       "        [    1,   450,  5637,  ...,   937,  3143,  1183],\n",
       "        [    1,   450,  5637,  ..., 10776,  1319,  2834],\n",
       "        [    1,   450,  5637,  ..., 18020,  6114,   471]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23551bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_cross_entropy(inputs, options, model, tokenizer):\n",
    "    \"\"\"\n",
    "    for HF models\n",
    "    \"\"\"\n",
    "    ce_list = []\n",
    "    for inp, out in zip(inputs, options):\n",
    "        prompt_input_ids = tokenizer.encode(inp, return_tensors='pt')\n",
    "\n",
    "        if model.config.is_encoder_decoder:\n",
    "            output_input_ids = tokenizer.encode(out, return_tensors='pt')\n",
    "            ce_loss = model(input_ids=prompt_input_ids, labels=output_input_ids)[0] * (output_input_ids.shape[1]-1)\n",
    "        else:    \n",
    "            full_input_ids = tokenizer.encode(inp + out, return_tensors='pt')\n",
    "            prompt_loss = model(prompt_input_ids, labels=prompt_input_ids)[0] * (prompt_input_ids.shape[1]-1)\n",
    "            full_loss = model(full_input_ids, labels=full_input_ids)[0] * (full_input_ids.shape[1]-1)\n",
    "            ce_loss = full_loss - prompt_loss\n",
    "        ce_loss = ce_loss.item()\n",
    "        ce_list.append(ce_loss)\n",
    "\n",
    "    return ce_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bab07a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.216720581054688, 15.125631332397461]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_cross_entropy(['Hello I am']*2, [' Bob', ' bye'], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc447d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4c7417b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "071d86a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1a05e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b0645471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5de191eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 22172,     2,     2,     2],\n",
       "        [    1, 22172,   920,   526,   366]]), 'attention_mask': tensor([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(['hello', 'hello how are you'], return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dddb40fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "442c6616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "db73ecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <unk>'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cd542e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.distractors import _replace_word_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdd9a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm afraid the cat was hit by a bmw.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_replace_word_in_sentence(\"I'm afraid the cat was hit by a car.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e557de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
