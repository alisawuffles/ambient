{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e18cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/xlab/alisaliu/ambient\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name != 'ambient':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74033923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from generation.gpt3_generation import request\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from utils.utils import predict_nli, strip_punctuation_and_casing\n",
    "from utils.constants import NLI_LABELS\n",
    "from mturk.utils import get_disambiguation_idxs\n",
    "import openai\n",
    "import truecase\n",
    "import string\n",
    "from sklearn.metrics import f1_score\n",
    "from evaluation.pmi import cross_entropy\n",
    "from evaluation.generative_evaluation import generative_evaluation\n",
    "from evaluation.continuation_evaluation import continuation_evaluation\n",
    "from evaluation.edit_f1 import get_edit_f1\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178f142",
   "metadata": {},
   "source": [
    "### evaluation for multilabel models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67764f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['roberta-large-rsnli-unli', 'roberta-large-smnli-ambnli', 'roberta-large-distilled-smnli', 'roberta-large-mnli', 'roberta-large-wanli', 'roberta-large-mchaos-multilabel', 'roberta-large-wanli-multilabel', 'roberta-large-wanli-set', 'binary_models']\n",
    "seeds = range(42, 47, 1)\n",
    "results = defaultdict(list)\n",
    "for seed in seeds:\n",
    "    for model in models:\n",
    "        with open(f'results/multilabel/{model}/seed{seed}.json') as fin:\n",
    "            res = json.load(fin)\n",
    "            results[model].append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d839b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-large-rsnli-unli\n",
      "EM 24.5 2.3\n",
      "Group EM 4.7 2.5\n",
      "F1 62.2 1.0\n",
      "Threshold contradiction 0.03 0.71\n",
      "Threshold entailment 0.69 1.0\n",
      "Threshold neutral 0.01 1.0\n",
      "\n",
      "\n",
      "roberta-large-smnli-ambnli\n",
      "EM 21.0 1.6\n",
      "Group EM 10.1 2.5\n",
      "F1 63.8 0.8\n",
      "Threshold -3.43\n",
      "\n",
      "\n",
      "roberta-large-distilled-smnli\n",
      "EM 24.3 1.1\n",
      "Group EM 4.7 1.2\n",
      "F1 68.0 0.1\n",
      "Threshold -1.55\n",
      "\n",
      "\n",
      "roberta-large-mnli\n",
      "EM 25.3 1.8\n",
      "Group EM 4.0 2.5\n",
      "F1 68.0 0.9\n",
      "Threshold -2.68\n",
      "\n",
      "\n",
      "roberta-large-wanli\n",
      "EM 30.8 3.8\n",
      "Group EM 10.1 7.9\n",
      "F1 71.4 0.3\n",
      "Threshold -1.19\n",
      "\n",
      "\n",
      "roberta-large-mchaos-multilabel\n",
      "EM 15.8 3.4\n",
      "Group EM 0.9 1.2\n",
      "F1 63.2 0.6\n",
      "Threshold -2.78\n",
      "\n",
      "\n",
      "roberta-large-wanli-multilabel\n",
      "EM 35.1 3.0\n",
      "Group EM 19.1 4.8\n",
      "F1 72.5 0.3\n",
      "Threshold -1.97\n",
      "\n",
      "\n",
      "roberta-large-wanli-set\n",
      "EM 43.6 0.8\n",
      "Group EM 37.8 0.4\n",
      "F1 70.7 0.2\n",
      "\n",
      "\n",
      "binary_models\n",
      "EM 37.6 3.1\n",
      "Group EM 20.0 6.2\n",
      "F1 72.3 0.6\n",
      "Threshold -2.22\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    model_df = pd.DataFrame(results[model])\n",
    "    print('EM', np.round(model_df.em.mean()*100, 1), np.round(model_df.em.std()*100, 1))\n",
    "    print('Group EM', np.round(model_df.group_em.mean()*100, 1), np.round(model_df.group_em.std()*100, 1))\n",
    "    print('F1', np.round(model_df.f1.mean()*100, 1), np.round(model_df.f1.std()*100, 1))\n",
    "    if 'logit_threshold' in model_df.columns:\n",
    "        if model_df.dtypes.logit_threshold == float:\n",
    "            print('Threshold', np.round(model_df.logit_threshold.median(), 2))\n",
    "        else:\n",
    "            for label in NLI_LABELS:\n",
    "                a = np.median([t[label][0] for t in model_df.logit_threshold])\n",
    "                b = np.median([t[label][1] for t in model_df.logit_threshold])\n",
    "                print('Threshold', label, np.round(a, 2), np.round(b, 2))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c2ddb",
   "metadata": {},
   "source": [
    "# 1. generative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec20766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan-t5-xxl 5.2\n",
      "llama-65b 10.0\n",
      "davinci 10.1\n",
      "text-davinci-003 14.5\n",
      "gpt-3.5-turbo 13.0\n",
      "gpt-4 18.0\n"
     ]
    }
   ],
   "source": [
    "models = ['flan-t5-xxl', 'llama-65b',  'davinci', 'text-davinci-003', 'gpt-3.5-turbo', 'gpt-4']\n",
    "num_incontext_examples = 4\n",
    "generative_results = {}\n",
    "dfs = []\n",
    "for model in models:\n",
    "    df = pd.read_json(f'results/generative_evaluation/{model}-n{num_incontext_examples}.jsonl', lines=True)\n",
    "    df['source'] = model\n",
    "    dfs.append(df)\n",
    "    print(model, np.round(df.edit_f1.mean()*100, 1))\n",
    "\n",
    "results_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49bca19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[DELETED]We're\", '[DELETED]afraid', '[DELETED]ambiguity', '[DELETED]stumps', '[DELETED]language', '[DELETED]models.', '[ADDED]He', '[ADDED]is', '[ADDED]so', '[ADDED]smart', '[ADDED]he', '[ADDED]could', '[ADDED]have', '[ADDED]been', '[ADDED]a', '[ADDED]doctor', '[ADDED]instead', '[ADDED]of', '[ADDED]his', '[ADDED]current', '[ADDED]occupation.']\n",
      "[\"[DELETED]We're\", '[DELETED]afraid', '[DELETED]ambiguity', '[DELETED]stumps', '[DELETED]language', '[DELETED]models.', '[ADDED]He', '[ADDED]is', '[ADDED]so', '[ADDED]smart', '[ADDED]he', '[ADDED]could', '[ADDED]have', '[ADDED]been', '[ADDED]a', '[ADDED]doctor,', '[ADDED]but', '[ADDED]he', '[ADDED]didn’t', '[ADDED]become', '[ADDED]one.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_edit_f1(\n",
    "    \"We're afraid that ambiguity stumps language models.\",\n",
    "    'He is so smart that he could have been a doctor instead of his current occupation.',\n",
    "    'He is so smart that he could have been a doctor, but he didn’t become one.',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67196c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(df):\n",
    "    num_total_disambiguations = 0\n",
    "    patterns = defaultdict(int)\n",
    "    for i, row in df.iterrows():\n",
    "        ambiguous_sentence_key = 'premise' if row['premise_ambiguous'] else 'hypothesis'\n",
    "        disambiguations = row['predicted_rewrites'].values()\n",
    "        if len(disambiguations) == 0:\n",
    "            patterns['empty'] += 1\n",
    "        \n",
    "        if all((row[ambiguous_sentence_key][:-1] in d) and (d != row[ambiguous_sentence_key]) for d in disambiguations):\n",
    "            patterns['restate_with_context'] += 1\n",
    "        \n",
    "        for disambiguation in disambiguations:\n",
    "            if disambiguation == row[ambiguous_sentence_key]:\n",
    "                patterns['copied'] += 1\n",
    "            num_total_disambiguations += 1\n",
    "    return {\n",
    "        'empty': np.round(patterns['empty'] / len(df) * 100, 1),\n",
    "        'copied': np.round(patterns['copied'] / num_total_disambiguations * 100, 1),\n",
    "        'restate_with_context': np.round(patterns['restate_with_context'] / len(df) * 100, 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9699e6f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan-t5-xxl {'empty': 41.7, 'copied': 30.3, 'restate_with_context': 46.2}\n",
      "llama-65b {'empty': 0.0, 'copied': 32.1, 'restate_with_context': 34.6}\n",
      "davinci {'empty': 0.0, 'copied': 25.4, 'restate_with_context': 39.3}\n",
      "text-davinci-003 {'empty': 0.2, 'copied': 3.1, 'restate_with_context': 38.5}\n",
      "gpt-3.5-turbo {'empty': 2.6, 'copied': 0.6, 'restate_with_context': 17.2}\n",
      "gpt-4 {'empty': 1.6, 'copied': 0.0, 'restate_with_context': 39.7}\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model_df = results_df.loc[results_df['source'] == model]\n",
    "    x = find_patterns(model_df)\n",
    "    print(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c8b9d",
   "metadata": {},
   "source": [
    "# 2. TF evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "334ddc08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- flan-t5-xxl ---\n",
      "Accuracy: 56.4\n",
      "EM Accuracy: 0.0\n",
      "0 85.9\n",
      "1 28.2\n",
      "2 100.0\n",
      "3 11.6\n",
      "prob mass of T/F tokens: 0.6931589238902599\n",
      "\n",
      "\n",
      "--- llama-65b ---\n",
      "Accuracy: 55.0\n",
      "EM Accuracy: 3.2\n",
      "0 96.1\n",
      "1 92.1\n",
      "2 11.8\n",
      "3 19.9\n",
      "prob mass of T/F tokens: 0.36481244281523867\n",
      "\n",
      "\n",
      "--- davinci ---\n",
      "Accuracy: 57.8\n",
      "EM Accuracy: 4.3\n",
      "0 46.2\n",
      "1 69.0\n",
      "2 45.0\n",
      "3 71.1\n",
      "prob mass of T/F tokens: 0.703112927097401\n",
      "\n",
      "\n",
      "--- text-davinci-003 ---\n",
      "Accuracy: 49.6\n",
      "EM Accuracy: 0.3\n",
      "0 71.9\n",
      "1 18.1\n",
      "2 81.0\n",
      "3 27.5\n",
      "prob mass of T/F tokens: 0.9952253991682005\n",
      "\n",
      "\n",
      "--- gpt-3.5-turbo ---\n",
      "Accuracy: 57.8\n",
      "EM Accuracy: 2.6\n",
      "0 81.5\n",
      "1 51.7\n",
      "2 74.5\n",
      "3 23.4\n",
      "prop of examples where T or F is top-1 token: 0.9974424552429667\n",
      "\n",
      "\n",
      "--- gpt-4 ---\n",
      "Accuracy: 63.0\n",
      "EM Accuracy: 2.5\n",
      "0 91.6\n",
      "1 68.8\n",
      "2 81.8\n",
      "3 9.9\n",
      "prop of examples where T or F is top-1 token: 0.9759164535379369\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = ['flan-t5-xxl', 'llama-65b', 'davinci', 'text-davinci-003', 'gpt-3.5-turbo', 'gpt-4']\n",
    "for model in models:\n",
    "    results_df = pd.read_json(f'results/TF_evaluation/{model}.jsonl', lines=True)\n",
    "    acc = (results_df.prediction == results_df.answer).sum()/len(results_df.index)\n",
    "    print(f'--- {model} ---')\n",
    "    print(f'Accuracy: {np.round(acc*100, 1)}')\n",
    "    \n",
    "    EM_accs = []\n",
    "    for (example_id, ambiguous_sentence_key, disambiguation), d_group in results_df.groupby(['example_id', 'ambiguous_sentence_key', 'disambiguation']):\n",
    "        if all(d_group.answer == d_group.prediction):\n",
    "            EM_accs.append(1)\n",
    "        else:\n",
    "            EM_accs.append(0)\n",
    "    \n",
    "    print(f'EM Accuracy: {np.round(np.mean(EM_accs)*100, 1)}')\n",
    "    \n",
    "    for template_id in results_df.template_id.unique():\n",
    "        template_df = results_df.loc[results_df['template_id'] == template_id]\n",
    "        print(template_id, np.round((template_df.prediction == template_df.answer).sum()/len(template_df.index)*100, 1))\n",
    "    \n",
    "    if 'TF_prob_mass' in results_df.columns:\n",
    "        prob_mass = results_df.TF_prob_mass.mean()\n",
    "        print(f'prob mass of T/F tokens: {prob_mass}')\n",
    "    else:\n",
    "        top_1_dict = results_df.prediction.value_counts().to_dict()\n",
    "        print(f'prop of examples where T or F is top-1 token: {(top_1_dict[True] + top_1_dict[False])/len(results_df.index)}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b1f16",
   "metadata": {},
   "source": [
    "### analysis: find self-contradictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84b833dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis\n",
    "model = 'gpt-4'\n",
    "results_df = pd.read_json(f'results/TF_evaluation/{model}.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f8a83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patterns = defaultdict(int)\n",
    "total_interps =0\n",
    "total_permutations = 0\n",
    "total_pairs = 0\n",
    "\n",
    "for (example_id, ambiguous_sentence_key), sentence_df in results_df.groupby(['example_id', 'ambiguous_sentence_key']):\n",
    "    disambiguation_dfs = sentence_df.groupby('disambiguation')\n",
    "    disambiguation_dfs = {k:v for k,v in disambiguation_dfs}\n",
    "    for disambiguation, df in disambiguation_dfs.items():\n",
    "        preds = df.prediction.tolist()\n",
    "        if preds[0] == True and preds[2] == True:\n",
    "            patterns['x1=T =><= x3=T (pure contradiction)'] += 1\n",
    "        if preds[1] == True and preds[3] == True:\n",
    "            patterns['x2=T =><= x4=T (pure contradiction)'] += 1\n",
    "        if (preds[0] == preds[3] == True) and (preds[1] == preds[2] == False):\n",
    "            patterns['x1=T, x2=F, x3=F, x4=T (sole interp)'] += 1\n",
    "        \n",
    "        total_interps += 1\n",
    "    \n",
    "    for pair in itertools.combinations(disambiguation_dfs.keys(), 2):\n",
    "        d1, d2 = pair[0], pair[1]\n",
    "        d1_preds = disambiguation_dfs[d1].prediction.tolist()\n",
    "        d2_preds = disambiguation_dfs[d2].prediction.tolist()\n",
    "        \n",
    "        if (d1_preds[0] == True and d1_preds[3] == True and d2_preds[0] == True):\n",
    "            patterns['x1=T, y1=T, x4=T, y4=T (symmetrical contradiction across interps)'] += 1\n",
    "        \n",
    "        total_pairs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e6ed293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'x1=T, x2=F, x3=F, x4=T (sole interp)': 333,\n",
       "             'x1=T, y1=T, x4=T, y4=T (symmetrical contradiction across interps)': 461,\n",
       "             'x2=T =><= x4=T (pure contradiction)': 647,\n",
       "             'x1=T =><= x3=T (pure contradiction)': 116})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff2911",
   "metadata": {},
   "source": [
    "# 3. continuation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3806680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan-t5-xxl 81.0\n",
      "llama-65b 68.9\n",
      "davinci 75.7\n",
      "text-davinci-003 71.4\n"
     ]
    }
   ],
   "source": [
    "models = ['flan-t5-xxl', 'llama-65b', 'davinci', 'text-davinci-003']\n",
    "for model in models:\n",
    "    df = pd.read_json(f'results/continuation_evaluation/{model}/results.jsonl', lines=True)\n",
    "    df['ranking'] = None\n",
    "    for i, row in df.iterrows():\n",
    "        KLs = {k:v['KL_div'] for k,v in row['options'].items()}\n",
    "        ranking = ' > '.join(sorted(KLs, key=KLs.get, reverse=True))\n",
    "        df.at[i, 'ranking'] = ranking\n",
    "    \n",
    "    correct_ranking_ct = np.sum([1 for r in df.ranking if r[0] == 'd'])\n",
    "    \n",
    "    print(model, np.round(correct_ranking_ct/len(df.index)*100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a40319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
