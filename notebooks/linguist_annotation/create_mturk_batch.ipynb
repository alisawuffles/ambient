{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbfc6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/xlab/alisaliu/ambignli/notebooks\n",
      "/mmfs1/gscratch/xlab/alisaliu/ambignli\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name != 'ambient':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbbe8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from modeling.multitask_model import RobertaForMultitaskSequenceClassification\n",
    "from utils.utils import predict_nli\n",
    "from utils.mturk_utils import read_batch\n",
    "from torch import sigmoid\n",
    "from collections import Counter\n",
    "from utils.utils import ensure_dir\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59a358",
   "metadata": {},
   "source": [
    "## find possibly ambiguous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_model = RobertaForSequenceClassification.from_pretrained('models/roberta-large-wanli-multilabel').to('cuda')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('models/roberta-large-wanli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca847de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_example_ambiguity(df):\n",
    "    df['ambiguity_score'] = None\n",
    "    df['predicted_labels'] = None\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df.index)):\n",
    "        premise, hypothesis = row['premise'], row['hypothesis']\n",
    "        probs = predict_nli(premise, hypothesis, multilabel_model, tokenizer)\n",
    "        preds = set([l for l, p in probs.items() if p > 0.04])\n",
    "        # ambiguity score is the probability assigned to the second-highest label\n",
    "        sorted_probs = sorted([p for p in probs.values()], reverse=True)\n",
    "        s = sorted_probs[1]\n",
    "        df.at[i, 'ambiguity_score'] = s\n",
    "        df.at[i, 'predicted_labels'] = ', '.join(sorted(preds))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921cb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dir = Path('generated_data/wanli_disagreement_p0.9_davinci-002')\n",
    "df_wanli_disagreement_instruct = pd.read_json(gen_dir / 'filtered_examples.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac77a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77564/77564 [18:52<00:00, 68.46it/s]\n"
     ]
    }
   ],
   "source": [
    "df = compute_example_ambiguity(df_wanli_disagreement_instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44964b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>nearest_neighbors</th>\n",
       "      <th>ambiguity_score</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The proposal was met with some skepticism from...</td>\n",
       "      <td>The proposal was met with some optimism from t...</td>\n",
       "      <td>[82936, 245722, 331487, 19994]</td>\n",
       "      <td>0.925016</td>\n",
       "      <td>contradiction, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The company's decision to downsize was met wit...</td>\n",
       "      <td>The company's decision to downsize was met wit...</td>\n",
       "      <td>[82936, 245722, 331487, 19994]</td>\n",
       "      <td>0.886042</td>\n",
       "      <td>contradiction, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The amount of money that was spent on the proj...</td>\n",
       "      <td>The amount of money that was saved on the proj...</td>\n",
       "      <td>[82936, 245722, 331487, 19994]</td>\n",
       "      <td>0.630185</td>\n",
       "      <td>contradiction, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We cannot be sure that the meeting will be pro...</td>\n",
       "      <td>We cannot be sure that the meeting will not be...</td>\n",
       "      <td>[82936, 245722, 331487, 19994]</td>\n",
       "      <td>0.126387</td>\n",
       "      <td>contradiction, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The company will only offer the position to so...</td>\n",
       "      <td>The company will only offer the position to so...</td>\n",
       "      <td>[214335, 8249, 65040, 102411]</td>\n",
       "      <td>0.791046</td>\n",
       "      <td>entailment, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77557</th>\n",
       "      <td>104063</td>\n",
       "      <td>Most people in the United States speak English.</td>\n",
       "      <td>English is the official language of the United...</td>\n",
       "      <td>[22805, 173022, 66665, 188215]</td>\n",
       "      <td>0.747514</td>\n",
       "      <td>entailment, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77558</th>\n",
       "      <td>104064</td>\n",
       "      <td>The novel is a fiction.</td>\n",
       "      <td>The movie is based on a true story.</td>\n",
       "      <td>[22805, 173022, 66665, 188215]</td>\n",
       "      <td>0.662432</td>\n",
       "      <td>contradiction, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77559</th>\n",
       "      <td>104065</td>\n",
       "      <td>The poet T.S. Eliot wrote, \"We shall not cease...</td>\n",
       "      <td>We never really know a place until we leave it.</td>\n",
       "      <td>[22805, 173022, 66665, 188215]</td>\n",
       "      <td>0.066416</td>\n",
       "      <td>contradiction, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77561</th>\n",
       "      <td>104067</td>\n",
       "      <td>The researchers say that this is the first stu...</td>\n",
       "      <td>This is the first study to look at the long-te...</td>\n",
       "      <td>[133594, 371112, 155042, 348420]</td>\n",
       "      <td>0.744364</td>\n",
       "      <td>entailment, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77562</th>\n",
       "      <td>104068</td>\n",
       "      <td>In a recent study, researchers found that peop...</td>\n",
       "      <td>Coffee is good for you.</td>\n",
       "      <td>[133594, 371112, 155042, 348420]</td>\n",
       "      <td>0.781466</td>\n",
       "      <td>entailment, neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35818 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            premise  \\\n",
       "0           0  The proposal was met with some skepticism from...   \n",
       "1           2  The company's decision to downsize was met wit...   \n",
       "2           3  The amount of money that was spent on the proj...   \n",
       "3           4  We cannot be sure that the meeting will be pro...   \n",
       "4           5  The company will only offer the position to so...   \n",
       "...       ...                                                ...   \n",
       "77557  104063    Most people in the United States speak English.   \n",
       "77558  104064                            The novel is a fiction.   \n",
       "77559  104065  The poet T.S. Eliot wrote, \"We shall not cease...   \n",
       "77561  104067  The researchers say that this is the first stu...   \n",
       "77562  104068  In a recent study, researchers found that peop...   \n",
       "\n",
       "                                              hypothesis  \\\n",
       "0      The proposal was met with some optimism from t...   \n",
       "1      The company's decision to downsize was met wit...   \n",
       "2      The amount of money that was saved on the proj...   \n",
       "3      We cannot be sure that the meeting will not be...   \n",
       "4      The company will only offer the position to so...   \n",
       "...                                                  ...   \n",
       "77557  English is the official language of the United...   \n",
       "77558                The movie is based on a true story.   \n",
       "77559    We never really know a place until we leave it.   \n",
       "77561  This is the first study to look at the long-te...   \n",
       "77562                            Coffee is good for you.   \n",
       "\n",
       "                      nearest_neighbors ambiguity_score  \\\n",
       "0        [82936, 245722, 331487, 19994]        0.925016   \n",
       "1        [82936, 245722, 331487, 19994]        0.886042   \n",
       "2        [82936, 245722, 331487, 19994]        0.630185   \n",
       "3        [82936, 245722, 331487, 19994]        0.126387   \n",
       "4         [214335, 8249, 65040, 102411]        0.791046   \n",
       "...                                 ...             ...   \n",
       "77557    [22805, 173022, 66665, 188215]        0.747514   \n",
       "77558    [22805, 173022, 66665, 188215]        0.662432   \n",
       "77559    [22805, 173022, 66665, 188215]        0.066416   \n",
       "77561  [133594, 371112, 155042, 348420]        0.744364   \n",
       "77562  [133594, 371112, 155042, 348420]        0.781466   \n",
       "\n",
       "             predicted_labels  \n",
       "0      contradiction, neutral  \n",
       "1      contradiction, neutral  \n",
       "2      contradiction, neutral  \n",
       "3      contradiction, neutral  \n",
       "4         entailment, neutral  \n",
       "...                       ...  \n",
       "77557     entailment, neutral  \n",
       "77558  contradiction, neutral  \n",
       "77559  contradiction, neutral  \n",
       "77561     entailment, neutral  \n",
       "77562     entailment, neutral  \n",
       "\n",
       "[35818 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres = 0.05\n",
    "sub_df = df.loc[df['ambiguity_score'] > thres]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d78824fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62807/2767759436.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  past_df = sub_df.loc[sub_df['id'].isin(old_ids)][~con_mask][ent_mask]\n",
      "/tmp/ipykernel_62807/2767759436.py:14: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  balanced_df = pd.concat([balanced_df, sub_df.loc[~sub_df['id'].isin(old_ids)][~con_mask][ent_mask].sample(num_entailment_needed-len(past_df))])\n"
     ]
    }
   ],
   "source": [
    "# include all examples with contradiction label\n",
    "con_mask = sub_df['predicted_labels'].str.contains('contradiction')\n",
    "balanced_df = sub_df[con_mask]\n",
    "# get label distribution\n",
    "counter = [ls.split(', ') for ls in balanced_df.predicted_labels.tolist()]\n",
    "counter = Counter([l for ls in counter for l in ls])\n",
    "# patch up with entailment examples\n",
    "num_entailment_needed = counter['contradiction'] - counter['entailment']\n",
    "ent_mask = sub_df['predicted_labels'].str.contains('entailment')\n",
    "balanced_df = pd.concat([balanced_df, sub_df[~con_mask][ent_mask].sample(num_entailment_needed)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e400a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contradiction, neutral                6850\n",
       "entailment, neutral                   6850\n",
       "contradiction, entailment, neutral    2531\n",
       "contradiction, entailment              595\n",
       "Name: predicted_labels, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.predicted_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a552c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.sample(frac=1).to_csv('annotation/AmbiEnt/balanced_examples.csv', index=False)\n",
    "balanced_df.sample(frac=1).to_json(gen_dir / 'balanced_examples.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f7062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
